{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACA-Workshop, Day 1\n",
    "## by Damian Trilling\n",
    "\n",
    "This is a Notebook file with exercises by a two-day workshop on using Python in the social sciences. It assumes that you have a very, very  basic understanding of Python (e.g., you know what a for-loop is). It introduces you into some basic techniques:\n",
    "- sentiment analysis\n",
    "- regular expressions (for, e.g., counting the occurrance of specific words)\n",
    "- basic natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "We assume that you have NLTK (Bird, Loper, & Klein, 2009) installed. If you use Anaconda, you have it anyway. Otherwise, use \n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "or \n",
    "```\n",
    "sudo pip install nltk\n",
    "```\n",
    "in your terminal to install it.\n",
    "Furthermore, we have to download some data for some specific NLTK modules. Download them by executing the following cell (you only have to do this once):\n",
    "\n",
    "Bird, S., Loper, E., & Klein, E. (2009). *Natural language processing with Python*. Sebastopol, CA: O'Reilly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming up\n",
    "\n",
    "Think back of what you know already about Python. Use the cell below to do the following task:\n",
    "- Create a list that contains strings with numbers inside, something like [\"12\",\"42\",\"11]\n",
    "- Write a loop that converts the strings to integers, prints them, and adds them to a new list\n",
    "- Modify your loop in such a way that it multiplies the numbers by two before adding them to the new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started!\n",
    "\n",
    "## Import modules\n",
    "Before we start, let's import some modules that we need today. It is good practice to do so at the beginning of a script, so we'll do it right now and not later when we need them. The benefit is that you immediately see if something goes wrong (for instance, because the module is not installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from nltk.sentiment import vader\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "We will use a dataset by Schumacher et al. (2016). From the abstract:\n",
    "> This paper presents EUSpeech, a new dataset of 18,403 speeches from EU leaders (i.e., heads of government in 10 member states, EU commissioners, party leaders in the European Parliament, and ECB and IMF leaders) from 2007 to 2015. These speeches vary in sentiment, topics and ideology, allowing for fine-grained, over-time comparison of representation in the EU. The member states we included are Czech Republic, France, Germany, Greece, Netherlands, Italy, Spain, United Kingdom, Poland and Portugal.\n",
    "\n",
    "Schumacher, G, Schoonvelde, M., Dahiya, T., Traber, D, & de Vries, E. (2016): *EUSpeech: a New Dataset of EU Elite Speeches*. [doi:10.7910/DVN/XPCVEI](http://dx.doi.org/10.7910/DVN/XPCVEI)\n",
    "\n",
    "Download and unpack the following file:\n",
    "```\n",
    "speeches_csv.tar.gz\n",
    "```\n",
    "\n",
    "In the .tar.gz file, you find a .zip file. Extract the whole folder to your home directory.\n",
    "See below a screenshot of how this looks like in Lubuntu (double-click on \"speeches_csv.zip\" in the left window, then the right window will open. Click on \"Extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"https://github.com/damian0604/bdaca/raw/master/ipynb/euspeech_download.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the files we downloaded. The following cell does this (assuming that you work on Linux or MacOS *and* that you saved the files in the same directory where you started your notebook server and where this notebook lies). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%ls Cleaned_Speeches/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some idea about the data\n",
    "Let us inspect the data. Let us only look at the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(\"Cleaned_Speeches/Speeches_NL_Cleaned.csv\") as fi:\n",
    "    reader=csv.reader(fi)\n",
    "    firstrow=next(reader)\n",
    "    print(\"It looks like we have\",len(firstrow),\"columns.\")\n",
    "    print(\"\\nThis is the content:\\n\")\n",
    "    print(firstrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can directly address a specific element from this row (we start counting at zero!). Which one might be most interesting for us? Just **play around** a bit! Note down (on a piece of paper or in a file) how the structure of the dataset looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firstrow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start!\n",
    "Now that we know how the data looks like, we can *loop* over all rows in the file in order to retrieve a list of all speeches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"Cleaned_Speeches/Speeches_NL_Cleaned.csv\") as fi:\n",
    "    reader=csv.reader(fi)\n",
    "    speeches=[]\n",
    "    for row in reader:\n",
    "        speeches.append(row[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll clean up a bit. You don't know the technique used here yet (it's called 'list comprehension), and I can explain it to you later. It is basically a short form of writing a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speeches=[speech.replace('<p>',' ').replace('</p>',' ') for speech in speeches]   #remove HTML tags\n",
    "speeches=[\" \".join(speech.split()) for speech in speeches]   # remove double spaces by splitting the strings into words and joining these words again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first speech to check everything's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speeches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "We will do our first analysis, using the algorithm by Hutto and Gilbert (2014). It is already implemented in NLTK, so we can run the analysis with just two lines of code! \n",
    "The only thing we have to care about is providing the input data and storing the output.\n",
    "\n",
    "Hutto, C.J., & Gilbert, E. (2014). Vader: A parsimonious rule-based model for sentiment analysis of social media text. *Eigth internatioanl AAAI conference on weblogs and social media.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti=vader.SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti.polarity_scores(speeches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti.polarity_scores(speeches[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how could we apply this to the whole dataset? With a loop! I'll give you a basic example with a lot of possibilities for improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"Cleaned_Speeches/Speeches_NL_Cleaned.csv\") as fi,  open('myoutput.csv',mode='w') as fo:\n",
    "    reader=csv.reader(fi)\n",
    "    writer=csv.writer(fo)\n",
    "    for row in reader:\n",
    "        speech=row[5]\n",
    "        sentiment = senti.polarity_scores(speech)\n",
    "        writer.writerow([speech[:100],sentiment['pos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head myoutput.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn!\n",
    "Your task: write a better code that \n",
    "- outputs more info\n",
    "- preprocesses the string (remove p-tags, for example)\n",
    "\n",
    "If you feel a bit more adventurous: \n",
    "- Add an if-statement to filter out the french speeches! Modify your script by including a structure like\n",
    "```\n",
    "if APPROPRIATECOLUMN=='en':\n",
    "    DO SOMETHING\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions\n",
    "There are a lot of online tutorials explaining regular expressions (and you can read up in my book or on the slides), so I won't go into detail here how to construct one. But let's look at a prototypical usecase: Counting how often something is mentioned in texts. Let's start by examing one single speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speeches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get a list with all substrings that match the regexp. And, as with any lists, we can calculate its length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.findall(r\"[Ee]conomy|[Ee]conomic\",speeches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(re.findall(r\"[Ee]conomy|[Ee]conomic\",speeches[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn!\n",
    "Let's write a loop to count the numbers of references to the economy per article and output it to a csv file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "As a prerequisite for many techiques we want to use tomorrow, we want to clean up the text. Typical steps involve:\n",
    "- converting to lowercase\n",
    "- remove punctuation\n",
    "- remove stopwords\n",
    "- stemming\n",
    "- parsing (= determining the grammatical function of words).\n",
    "Of course, depending on the task at hand, we don't want to do all of them - and also the order matters. If we want to parse a sentence, well, we better still have a sentence (and not already have removed stopwords and punctuation).\n",
    "\n",
    "Below, you find some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanedspeeches=[]\n",
    "for speech in speeches:\n",
    "    speech=speech.lower().replace(\".\",\"\").replace(\",\",\"\").replace('\"',''.replace(\"'\",\"\")).replace(\"?\",\"\")\n",
    "    words=speech.split()\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    speechnew = \" \".join(words)\n",
    "    cleanedspeeches.append(speechnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanedspeeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and retaining only nouns and adjectives\n",
    "Look at the NLTK documentation to find out what each code means (e.g., 'NN' is 'noun') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speechesnounsadj=[]\n",
    "for speech in speeches:\n",
    "    tokens = nltk.word_tokenize(speech)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    cleanspeech = \"\"\n",
    "    for element in tagged:\n",
    "        if element[1] in ('NN','NNP','JJ'):\n",
    "            cleanspeech=cleanspeech+element[0]+\" \"\n",
    "    speechesnounsadj.append(cleanspeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speechesnounsadj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
