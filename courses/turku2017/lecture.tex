% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass{beamer}
% for handouts: \documentclass[handout]{beamer}

%\setbeamertemplate{background canvas}[vertical shading][bottom=white,top=structure.fg!25]
% or whatever

\usetheme[compress]{Amsterdam}
%\setbeamertemplate{headline}{}
%\setbeamertemplate{footline}{}
%\setbeamersize{text margin left=0.5cm}
  
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{multicol}



\usepackage{color}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\lstset{
basicstyle=\scriptsize\ttfamily,
columns=flexible,
breaklines=true,
numbers=left,
%stepsize=1,
numberstyle=\tiny,
backgroundcolor=\color[rgb]{0.85,0.90,1}
}


\begin{document}

\title{Using computational methods to analyze communication}
\author[Damian Trilling]{Damian Trilling \\ ~ \\ \footnotesize{d.c.trilling@uva.nl \\@damian0604} \\ \url{www.damiantrilling.net}}
\date{27--11--2017}
\institute[UvA]{Afdeling Communicatiewetenschap \\Universiteit van Amsterdam}



\begin{frame}{}
\titlepage
\end{frame}

\begin{frame}{Today}
\tableofcontents
\end{frame}



\section[What's ACA?]{What's Automated Content Analysis?}
\begin{frame}[plain]
	What's Automated Content Analysis?
\end{frame}


\begin{frame}[plain]
\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{boumanstrilling2016}
\label{fig:boumanstrilling2016}
\end{figure}
\tiny{Boumans, J. W., \& Trilling, D. (2016). Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars. \emph{Digital Journalism, 4}(1), 8–23. doi:10.1080/21670811.2015.1096598}
\end{frame}




\section[Basic ACA]{Basic ACA: Dictionary- and string-based methods}

\subsection{Regular expressions}
\begin{frame}
	\textbf{Basic ACA: Dictionary- and string-based methods}\\
	Regular expressions
\end{frame}


\begin{frame}{Regular Expressions: What and why?}
\begin{block}{What is a regexp?}
\begin{itemize}
\item<1-> a \emph{very} widespread way to describe patterns in strings
\item<2-> Think of wildcards like {\tt{*}} or operators like {\tt{OR}}, {\tt{AND}} or {\tt{NOT}} in search strings: a regexp does the same, but is \emph{much} more powerful
\item<3-> You can use them in many text editors (!), in STATA, R, Python, \ldots 
\end{itemize}
\end{block}
\end{frame}o

\begin{frame}{An example}
\begin{block}{We wanted to find references to companies in several years of news coverage}
Problems: 
\begin{itemize}
\item Spelling variations (ABN, ABN Amro, ABN-Amro, \ldots)
\item Shouldn't be in the middle of the word, but \emph{can} be at the beginning of a word, optionally connected with a hyphen (``ABN-topman'', ``Shellstation'')
\end{itemize}
For instance, \\
{\texttt{\textbackslash bING(?:-.*?)?\textbackslash b}} \\
allows to specify exactly this.
\end{block}
{\tiny{Strycharz, J., Strauss, N., \& Trilling, D. (2017). The role of media coverage in explaining stock market fluctuations: insights for strategic financial communication. \textit{International Journal of Strategic Communication, online first}. doi:10.1080/1553118X.2017.1378220 \\
Jonkman, J. G., Trilling, D., Verhoeven, P., \& Vliegenthart, R. (2016). More or less diverse: An assessment of the effect of attention to media salient company types on media agenda diversity in Dutch newspaper coverage between 2007 and 2013.\textit{ Journalism, online first.} doi:10.1177/1464884916680371\\ } }
\end{frame}

\begin{frame}{Basic regexp elements}
\begin{block}{Alternatives}
\begin{description}
\item[{\tt{\lbrack TtFf\rbrack}}] matches either T or t or F or f
\item[{\tt{Twitter|Facebook}}] matches either Twitter or Facebook
\item[{\tt{.}}] matches any character
\end{description}
\end{block}
\begin{block}{Repetition}<2->
\begin{description}
\item[{\tt{*}}] the expression before occurs 0 or more times
\item[{\tt{+}}] the expression before occurs 1 or more times
\end{description}
\end{block}
\end{frame}

\begin{frame}{regexp quizz}
\begin{block}{Which words would be matched?}
\tt
\begin{enumerate}
\item<1-> \lbrack Pp\rbrack ython
\item<2-> \lbrack A-Z\rbrack +
\item<3-> RT :* @\lbrack a-zA-Z0-9\rbrack *
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{What else is possible?}
If you google {\tt{regexp}} or {\tt{regular expression}}, you'll get a bunch of useful overviews. The wikipedia page is not too bad, either. 
\end{frame}

%
%\begin{frame}{How to use regular expressions in Python}
%\begin{block}{The module re}
%\begin{description}
%\item<1->[{\tt{re.findall("\lbrack Tt\rbrack witter|\lbrack Ff\rbrack acebook",testo)}}] returns a list with all occurances of Twitter or Facebook in the string called {\tt{testo}}
%\item<1->[{\tt{re.findall("\lbrack 0-9\rbrack +\lbrack a-zA-Z\rbrack +",testo)}}] returns a list with all words that start with one or more numbers followed by one or more letters in the string called {\tt{testo}}
%\item<2->[{\tt{re.sub("\lbrack Tt\rbrack witter|\lbrack Ff\rbrack acebook","a social medium",testo)}}] returns a string in which all all occurances of Twitter or Facebook are replaced by "a social medium"
%\end{description}
%\end{block}
%\end{frame}
%
%
%\begin{frame}[fragile]{How to use regular expressions in Python}
%\begin{block}{The module re}
%\begin{description}
%\item<1->[{\tt{re.match(" +(\lbrack 0-9\rbrack +) of (\lbrack 0-9\rbrack +) points",line)}}] returns  \texttt{None} unless it \emph{exactly} matches the string \texttt{line}. If it does, you can access the part between () with the \texttt{.group()} method.
%\end{description}
%\end{block}
%
%Example:
%\begin{lstlisting}
%line="             2 of 25 points"
%result=re.match(" +([0-9]+) of ([0-9]+) points",line)
%if result:
%   print ("Your points:",result.group(1))
%   print ("Maximum points:",result.group(2))
%\end{lstlisting}
%Your points: 2\\
%Maximum points: 25
%\end{frame}














\begin{frame}{Possible applications}
\begin{block}{Data preprocessing}
\begin{itemize}
\item Remove unwanted characters, words, \ldots
\item Identify \emph{meaningful} bits of text: usernames, headlines, where an article starts, \ldots
\item filter (distinguish relevant from irrelevant cases)
\end{itemize}
\end{block}
\end{frame}


\begin{frame}{Possible applications}
\begin{block}{Data analysis: Automated coding}
\begin{itemize}
\item Actors
\item Brands
\item links or other markers that follow a regular pattern
\item Numbers (!)
\end{itemize}
\end{block}
\end{frame}




\section{Unsupervised Machine Learning}

\begin{frame}
	Unsupervised Machine Learning
\end{frame}



\begin{frame}[plain]
	\begin{figure}
		\centering
		\includegraphics[width=1.0\linewidth]{boumanstrilling2016}
		\label{fig:boumanstrilling2016}
	\end{figure}
	\tiny{Boumans, J. W., \& Trilling, D. (2016). Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars. \emph{Digital Journalism, 4}(1), 8–23. doi:10.1080/21670811.2015.1096598}
\end{frame}




\begin{frame}{Supervised vs. unsupervised learning}
	\begin{columns}[t]
		
		\column{.5\textwidth}
		
		\begin{block}{Unsupervised}<1->
			\begin{itemize}
				\item No manually coded data
				\item We want to identify patterns or to make groups of most similar cases
				%\item Per case, we want to know to which group it belongs
			\end{itemize}
		\end{block}
		{\footnotesize{
				\onslide<2->{Example: We have a dataset of Facebook-massages on an organizations' page. We use clustering to group them and later interpret these clusters (e.g., as complaints, questions, praise, \ldots)}
			}}
			
			\column{.5\textwidth}
			\begin{block}{Supervised}<3->
				\begin{itemize}
					\item We code a small dataset by hand and use it to ``train'' a machine
					\item The machine codes the rest 
				\end{itemize}
			\end{block}
			
			{\footnotesize{
					\onslide<4->{Example: %We use a hand-coded CSV table with two columns (tweet and gender of the sender) as training dataset and then predict for a different dataset per tweet if it was sent by a man or a woman.
						We have 2,000 of these messages grouped into such categories by human coders. We then use this data to group all remaining messages as well.
					}
				}}
				
			\end{columns}
		\end{frame}
		
		



\begin{frame}[plain]
	inductive and bottom-up:\\ \textbf{unsupervised machine learning}\\
	\vspace{1cm}\hspace{1cm} \onslide<2> \footnotesize{(something you aready did in your Bachelor -- no kidding.)}
\end{frame}



\subsection{PCA}




\begin{frame}{Principal Component Analysis? How does \emph{that} fit in here?}
	\onslide<2>{In fact, PCA is used everywhere, even in image compression}
	
	\begin{block}<3->{PCA in ACA}
		\begin{itemize}
			\item Find out what word cooccur (inductive frame analysis)
			\item Basically, transform each document in a vector of word frequencies and do a PCA
		\end{itemize}
	\end{block}
	%\onslide<4>{\textbf{But we'll look at the state of the art instead: Latent Dirichlet Allication (LDA)}}
\end{frame}

\begin{frame}[fragile]{A so-called term-document-matrix}
\begin{lstlisting}
      w1,w2,w3,w4,w5,w6 ...
text1, 2, 0, 0, 1, 2, 3 ...
text2, 0, 0, 1, 2, 3, 4 ...
text3, 9, 0, 1, 1, 0, 0 ...
...
\end{lstlisting}
\vspace{1cm}
\onslide<2>{These can be simple counts, but also more advanced metrics, like tf-idf scores (where you weigh the frequency by the number of documents in which it occurs), cosine distances, etc.}
\end{frame}


\begin{frame}{PCA: implications and problems}
	\begin{itemize}
		\item given a term-document matrix, easy to do with any tool
		\item probably extremely skewed distributions
		\item some problematic assumptions: does the goal of PCA, to find a solution in which one word loads on \emph{one} component match real life, where a word can belong to several topics or frames?
	\end{itemize}
\end{frame}





\subsection{LDA}


\begin{frame}{}
	Enter \textbf{topic modeling with Latent Dirichlet Allocation (LDA)}
\end{frame}








\begin{frame}{LDA, what's that?}
	\begin{block}{No mathematical details here, but the general idea}
		\begin{itemize}
			\item There are $k$ topics, $T_1$\ldots$T_k$
			\item Each document $D_i$ consists of a mixture of these topics, e.g.$80\% T_1, 15\% T_2, 0\% T_3, \ldots 5\% T_k $
			\item On the next level, each topic consists of a specific probability distribution of words
			\item Thus, based on the frequencies of words in $D_i$, one can infer its distribution of topics
			\item Note that LDA (like PCA) is a Bag-of-Words (BOW) approach
		\end{itemize}
	\end{block}
	
\end{frame}




\begin{frame}[fragile]{Doing a LDA in Python}
You can use gensim ({\v R}eh{\r u}{\v r}ek \& Sojka, 2010) for this.
%
%Let us assume you have a list of lists of words (!) called \texttt{texts}:
%
%\begin{lstlisting}
%articles=['The tax deficit is higher than expected. This said xxx ...', 'Germany won the World Cup. After a']
%texts=[art.split() for art in articles]
%\end{lstlisting}
%which looks like this:
%\begin{lstlisting}
%[['The', 'tax', 'deficit', 'is', 'higher', 'than', 'expected.', 'This', 'said', 'xxx', '...'], ['Germany', 'won', 'the', 'World', 'Cup.', 'After', 'a']]
%\end{lstlisting}

\tiny{{\v R}eh{\r u}{\v r}ek, R., \& Sojka, P. (2010). Software framework for topic modelling with large corpora. \emph{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}, pp. 45–50. Valletta, Malta: ELRA. }
	
\end{frame}




\begin{frame}[plain,fragile]
\begin{lstlisting}
from gensim import corpora, models

NTOPICS = 100
LDAOUTPUTFILE="topicscores.tsv"

# Create a BOW represenation of the texts
id2word = corpora.Dictionary(texts)
mm =[id2word.doc2bow(text) for text in texts]

# Train the LDA models.
lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=NTOPICS)

# Print the topics.
for top in lda.print_topics(num_topics=NTOPICS, num_words=5):
    print ("\n",top)

# save topic scores
scoresperdoc=lda.inference(mm)
with open(LDAOUTPUTFILE,"w",encoding="utf-8") as fo:
  for row in scoresperdoc[0]:
    fo.write("\t".join(["{:0.3f}".format(score) for score in row]))
    fo.write("\n")
\end{lstlisting}

\end{frame}




\begin{frame}[fragile]{Output: Topics (below) \& topic scores (next slide)}
\begin{lstlisting}
0.069*fusie + 0.058*brussel + 0.045*europesecommissie + 0.036*europese + 0.023*overname
0.109*bank + 0.066*britse + 0.041*regering + 0.035*financien + 0.033*minister
0.114*nederlandse + 0.106*nederland + 0.070*bedrijven + 0.042*rusland + 0.038*russische
0.093*nederlandsespoorwegen + 0.074*den + 0.036*jaar + 0.029*onderzoek + 0.027*raad
0.099*banen + 0.045*jaar + 0.045*productie + 0.036*ton + 0.029*aantal
0.041*grote + 0.038*bedrijven + 0.027*ondernemers + 0.023*goed + 0.015*jaar
0.108*werknemers + 0.037*jongeren + 0.035*werkgevers + 0.029*jaar + 0.025*werk
0.171*bank + 0.122* + 0.041*klanten + 0.035*verzekeraar + 0.028*euro
0.162*banken + 0.055*bank + 0.039*centrale + 0.027*leningen + 0.024*financiele
0.052*post + 0.042*media + 0.038*nieuwe + 0.034*netwerk + 0.025*personeel
...
\end{lstlisting}
\end{frame}


\begin{frame}[plain]
	\makebox[\linewidth]{
		\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{topicscores}}
\end{frame}




\begin{frame}[fragile]{Visualization with pyldavis}
\begin{lstlisting}
import pyLDAvis
import pyLDAvis.gensim
% first estiate gensim model, then:
vis_data = pyLDAvis.gensim.prepare(lda,mm,id2word)
pyLDAvis.display(vis_data)
\end{lstlisting}
\makebox[\linewidth]{
	\includegraphics[width=\paperwidth,height=.5\paperheight,keepaspectratio]{pyldavis}}
\end{frame}





\section{Supervised Machine Learning}

\begin{frame}[plain]
	predefined categories, but no predefined rules:\\ \textbf{supervised machine learning}\\
	\vspace{1cm}\hspace{1cm} \onslide<2> \footnotesize{(something you aready did in your Bachelor -- no kidding.)}
\end{frame}





\begin{frame}[plain]
	\begin{figure}
		\centering
		\includegraphics[width=1.0\linewidth]{boumanstrilling2016}
		\label{fig:boumanstrilling2016}
	\end{figure}
	\tiny{Boumans, J. W., \& Trilling, D. (2016). Taking stock of the toolkit: An overview of relevant autmated content analysis approaches and techniques for digital journalism scholars. \emph{Digital Journalism, 4}(1), 8–23. doi:10.1080/21670811.2015.1096598}
\end{frame}



		
		\subsection{You have done it before!}
		\begin{frame}{You have done it before!}
			\begin{block}{Regression}<2->
				\begin{enumerate}
					\item<3-> Based on your data, you estimate some regression equation 	$y_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \varepsilon_i$
					\item<4-> Even if you have some \emph{new unseen data}, you can estimate your expected outcome $\hat{y}$!
					\item<5-> Example: You estimated a regression equation where $y$ is newspaper reading in days/week: $y = -.8 + .4 \times man + .08 \times age$
					\item<6-> You could now calculate $\hat{y}$ for a man of 20 years and a woman of 40 years -- \emph{even if no such person exists in your dataset}: \\
					$\hat{y}_{man20} = -.8 + .4 \times 1 + .08 \times 20 = 1.2$ \\
					$\hat{y}_{woman40} = -.8 + .4 \times 0 + .08 \times 40 = 2.4$
				\end{enumerate}
			\end{block}	
			
		\end{frame}
		
		
		\begin{frame}{}
			\huge{This is\\ Supervised Machine Learning!}
		\end{frame}
		
		\begin{frame}{\ldots but\ldots}
			\begin{itemize}
				\item<1-> We will only use \emph{half} {\tiny{(or another fraction)}} of our data to estimate the model, so that we can use the other half to check if our predictions match the manual coding (``labeled data'',``annotated data'' in SML-lingo)
				\begin{itemize}
					\item<2->e.g., 2000 labeled cases, 1000 for training, 1000 for testing --- if successful, run on 100,000 unlabeled cases
				\end{itemize}
				\item<3-> We use many more independent variables (``features'')
				\item<4-> Typically, IVs are word frequencies (often weighted, e.g. tf$\times$idf) ($\Rightarrow$BOW-representation)
			\end{itemize}
		\end{frame}
		
		
		\subsection{Applications}
		
		\begin{frame}{Applications}
			\begin{block}<2->{In other fields}
				\emph{A lot} of different applications
				\begin{itemize}
					\item from recognizing hand-written characters to recommendation systems
				\end{itemize}
			\end{block}
			
			\begin{block}<3>{In our field}
				It starts to get popular to measure latent variables
				\begin{itemize}
					\item frames
					\item topics
				\end{itemize}
			\end{block}
		\end{frame}
		
		
		
		\begin{frame}{SML to code frames and topics}
			\begin{block}{Some work by Burscher and colleagues}
				\begin{itemize}
					\item Humans can code generic frames (human-interest, economic, \ldots)
					\item Humans can code topics from a pre-defined list 
					\item<2->\textbf{But it is very hard to formulate an explicit rule} \\(as in: code as 'Human Interest' if regular expression R is matched)
				\end{itemize}
				\onslide<3>$\Rightarrow$ This is where you need supervised machine learning!
			\end{block}
			\tiny{Burscher, B., Odijk, D., Vliegenthart, R., De Rijke, M., \& De Vreese, C. H. (2014). Teaching the computer to code frames in news: Comparing two supervised machine learning approaches to frame analysis. \emph{Communication Methods and Measures, 8}(3), 190–206. doi:10.1080/19312458.2014.937527
				\\
				Burscher, B., Vliegenthart, R., \& De Vreese, C. H. (2015). Using supervised machine learning to code policy issues: Can classifiers generalize across contexts? \emph{Annals of the American Academy of Political and Social Science, 659}(1), 122–131.
			}
			
		\end{frame}
		
		
		
		
		
		{\setbeamercolor{background canvas}{bg=black}
			\begin{frame}[plain]
				\makebox[\linewidth]{
					\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{burscher2014}}
			\end{frame}
			
			\begin{frame}[plain]
				\makebox[\linewidth]{
					\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{burscher2015-a}}
			\end{frame}
			
			\begin{frame}[plain]
				\makebox[\linewidth]{
					\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{burscher2015-b}}
			\end{frame}
		}
		
		
		\begin{frame}[plain]
			\begin{columns}[]
				\column{.5\textwidth}
				
				{\tiny{http://commons.wikimedia.org/wiki/File:Precisionrecall.svg}}
				\makebox[\linewidth]{
					\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{precisionrecall.png}}
				
				\column{.5\textwidth}
				\begin{block}{Some measures of accuracy}
					\begin{itemize}
						\item Recall
						\item Precision
						\item $\text{F1}=2\cdot \frac{\text{precision}\cdot \text{recall}}{\text{precision}+\text{recall}}$
						\item AUC (Area under curve) $[0,1]$, $0.5=$ random guessing
					\end{itemize}
				\end{block}
				
				
			\end{columns}
			
		\end{frame}
		
		
		
		
		
		\begin{frame}{What does this mean for our research?}
			\begin{block}<2>{It we have 2,000 documents with manually coded frames and topics\ldots}
				\begin{itemize}
					\item we can use them to train a SML classifier
					\item which can code an unlimited number of new documents
					\item with an acceptable accuracy
				\end{itemize}
			\end{block}
			\onslide<2>{
				\tiny{Some easier tasks even need only 500 training documents, see Hopkins, D. J., \& King, G. (2010). A method of automated nonparametric content analysis for social science. \emph{American Journal of Political Science, 54}(1), 229–247.}} 
		\end{frame}
		
		

%		
%		\begin{frame}[fragile]{An implementation}
%			Let's say we have a list of tuples with movie reviews and their rating:
%			\begin{lstlisting}
%			reviews=[("This is a great movie",1),("Bad movie",-1), ... ...]
%			\end{lstlisting}
%			And a second list with an identical structure:
%			\begin{lstlisting}
%			test=[("Not that good",-1),("Nice film",1), ... ...]
%			\end{lstlisting}
%			Both are drawn from the same population, it is pure chance whether a specific review is on the one list or the other.\\
%			\tiny{Based on an example from \url{http://blog.dataquest.io/blog/naive-bayes-movies/}}
%		\end{frame}
%		
%		
%		\begin{frame}[fragile]{Training a A Na\"ive Bayes Classifier}
%			\begin{lstlisting}
%			from sklearn.naive_bayes import MultinomialNB
%			from sklearn.feature_extraction.text import CountVectorizer
%			from sklearn import metrics
%			
%			# This is just an efficient way of computing word counts
%			vectorizer = CountVectorizer(stop_words='english')
%			train_features = vectorizer.fit_transform([r[0] for r in reviews])
%			test_features = vectorizer.transform([r[0] for r in test])
%			
%			# Fit a naive bayes model to the training data.
%			nb = MultinomialNB()
%			nb.fit(train_features, [r[1] for r in reviews])
%			
%			# Now we can use the model to predict classifications for our test features.
%			predictions = nb.predict(test_features)
%			actual=[r[1] for r in test]
%			
%			# Compute the error.
%			fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)
%			print("Multinomal naive bayes AUC: {0}".format(metrics.auc(fpr, tpr)))
%			
%			\end{lstlisting}
%		\end{frame}
		%
		%\begin{frame}{TODO}
		%TODO\\
		%andere vectorizer (TFIDF)\\
		%verschillen classifiers\\
		%andere output (metrics summary)
		%\\
		%waarom is dit hier een MULTINOMIAL NB
		%\\
		%scikit-learn installeren\\ 
		%opdracht bedenken: classifiers vergelijken
		%\end{frame}
		
	
		\begin{frame}{Different vectorizers}
			\begin{itemize}
				\item CountVectorizer (=simple word counts)
				\item TfidfVectorizer (word counts (``term frequency'') weighted by number of documents in which the word occurs at all (``inverse document frequency''))
				\item additional options: stopwords, thresholds for minimum frequencies etc.
			\end{itemize}
		\end{frame}
		
		\begin{frame}{Different classifiers}
			\begin{itemize}
				\item Naïve Bayes
				\item Logistic Regression
				\item Support Vector Machine (SVM)
				\item \ldots
			\end{itemize}
			Typical approach: Find out which setup performs best (see example source code in the book).
		\end{frame}
		
	
\section{Examples}
\begin{frame}
	\textbf{Examples} \\
	Putting these techniques into practice
	\vspace{1cm}
	\begin{itemize}
		\item Shareworthiness
		\item Online vs offline news
	\end{itemize}
\end{frame}



\subsection{Shareworthiness}

\begin{frame}{What explains how often an article is shared on social media?}
Trilling, D., Tolochko, P., \& Burscher, B. (2017). From Newsworthiness to Shareworthiness. \textit{Journalism \& Mass Communication Quarterly, 94}(1), 38–60. doi:10.1177/1077699016654682
\end{frame}


\begin{frame}{The research design}
\begin{block}{The data}
	\begin{itemize}
		\item Subscribe to RSS feeds of major news outlets
		\item Query feeds 1x/hour for a year, follow links and download
		\item Parse downloads (i.e., extract title, text, \ldots)
		\item Use Twitter and Facebook API to retrieve number of shares
	\end{itemize}
\end{block}
\end{frame}



\begin{frame}{The research design}
	\begin{block}{The automated content analysis}
		\begin{itemize}
		\item written by press agency?: regular expressions
		\item geographical location: regular expressions
		\item positivity/negativity: sentiment analysis package
		\item topic: supervised machine learning
		\item economic and human-interest frames: supervised machine learning
		\item topic popularity: part-of-speech tagging, calculation of overlap of nouns in time frame
		\end{itemize}
	\end{block}
\end{frame}



\begin{frame}{The research design}
	\begin{block}{The final models}
		\begin{itemize}
		\item negative binomial regression to predict the number of shares
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}{What did we find?}
	\begin{itemize}
		\item it's not true that mostly soft topics are shared
		\item geographical closeness matters
		\item differences between Facebook and Twitter (e.g., more skewed towards popular stories on FB, more long tail on Twitter)
	\end{itemize}
\end{frame}


\subsection{Difference between offline and online news}
\begin{frame}{How do online and offline news differ?}
Burggraaff, C., \& Trilling, D. (2017). Through a different gate: An automated content analysis of how online news and print news differ. \textit{Journalism, online first}. doi:10.1177/1464884917716699
\end{frame}


\begin{frame}{The research design}
	\begin{block}{The data}
		\begin{itemize}
		\item Online: as in previous study, but longer time period
		\item Plus offline articles from a newspaper database
		\end{itemize}
	\end{block}
\end{frame}


\begin{frame}{The research design}
	\begin{block}{The automated content analysis}
	\begin{itemize}
	\item As in previous study, additionally:
	\item Follow-up news?: cosine similarity
	\item References to persons: Named entity recognition (NER)
	\item Entertainment news: supervised machine learning
	\item Celebrity news: NER + SPARQL-queries on DBpedia (=Wikipedia)
	\end{itemize}
	\end{block}
\end{frame}



\begin{frame}{The research design}
	\begin{block}{The final models}
		\begin{itemize}
			\item regression models that predict the presence of news values (based on among other things online/offline-dummy)
		\end{itemize}
	\end{block}
\end{frame}


\begin{frame}{What did we find?}
	\begin{itemize}
	\item significant differences between online and offline
	\item e.g., online more follow-up
	\item but: no evidence for common perception that online is more entertainment and celebrity news
	\end{itemize}
\end{frame}




\section{Take-home message}
\begin{frame}
	Take-home message
\end{frame}

\begin{frame}{Take-home message}
\begin{itemize}
	\item There is more than one form of ACA
	\item top-down (deductive) vs bottom-up (inductive)
	\item You can start simple!
	\item No need to use specialized software, it's all available in Python (or R)
\end{itemize}
\end{frame}


\begin{frame}{Tomorrow}
	Workshop: A bit of ACA in Python
\end{frame}

\begin{frame}[plain]
	\huge
	\centering
	Damian Trilling\\ \vspace{0.5cm}
	d.c.trilling@uva.nl\\
	@damian0604\\
	www.damiantrilling.net\\
\end{frame}


\end{document}


