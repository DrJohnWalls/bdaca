% !TEX encoding = UTF-8 Unicode

\documentclass[a4paper,12pt]{report}
\usepackage[natbibapa,nosectionbib,tocbib,numberedbib]{apacite}
\AtBeginDocument{\renewcommand{\bibname}{Literature}}


\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{url}

\usepackage[colorinlistoftodos]{todonotes}

\usepackage{pifont}

\usepackage{lmodern}
\usepackage{listings}
\lstset{
basicstyle=\scriptsize\ttfamily,
columns=flexible,
breaklines=true,
numbers=left,
%stepsize=1,
numberstyle=\tiny,
backgroundcolor=\color[rgb]{0.85,0.90,1}
}


\let\oldquote\quote
\let\endoldquote\endquote
\renewenvironment{quote}{\footnotesize\oldquote}{\endoldquote}



\title{Four-day workshop Automated Content Analysis with Python\\~\\Course Manual}
\author{dr. Damian Trilling\\~\\
Department of Communication Science
\\University of Amsterdam\\~\\d.c.trilling@uva.nl\\www.damiantrilling.net\\@damian0604\\}
\date{11/12 \& 25/26 September 2017}


\begin{document}
\maketitle

%\tableofcontents


\section*{About this course}

\subsection*{Course description}
 
In the social sciences, there is an increasing interest for automatically analyzing texts. This in particular concerns research that draws on Internet-based data sources such as social media, online news, large digital archives, and public comments to news and products.  This emerging field of studies is also called \emph{Computational Social Science} \citep{Lazer2009} or even \emph{Computational Communication Science} \citep{Shah2015}.

The workshop will provide insights in the basic concepts, challenges and opportunities associated with data so large that traditional research methods (like manual coding) cannot be applied any more. Participants are introduced to strategies and techniques for analyzing large quantities of text, through concrete examples and templates than can be shared and modified for own research projects. 


\subsection*{Goals}
Upon completion of this course, the following goals are reached:
\begin{enumerate}[A]

\item Participants can identify research methods from computer science and computational linguistics which can be used for research in the domain of the social sciences; they can explain the principles of these methods and apply them to the analysis of texts.

\item Participants have a basic knowledge of the Python programming language and can work with Jupyter Notebooks.

\item Participants can at least on a basic level implement techniques from (A), using commonly used Python modules.

\end{enumerate}


\subsection*{Readings}
We will work with the book by \cite{Trilling2016}, which can be downloaded as a PDF file (see references).


In the schedule below, I refer to chapters from this book, where you can find a bit more background and explanation about the topics we will discuss..


\section*{Preparation and Prerequisits}
\textsc{\ding{52} Chapter 1: Preparing your computer} \textbf{\emph{or (!)}} install Anaconda\\
~\\
You are expected to bring a laptop with either Anaconda (Python 3 version) \emph{or} a virtual machine as explained in Chapter~1 of the book installed. Anaconda can be downloaded from  \url{https://www.continuum.io/downloads}.










\section*{Monday, 11-7-2017}
\subsection*{Morning (9.30--12.30)}
\textsc{\ding{52} Chapter 3: A language, not a program}, in particular: \\ \textsc{\ding{52} Section 3.5: Jupyter Notebook} \\
\textsc{\ding{52} Section 4.1: Data types} \\
~\\
After a short introduction round, we will discuss some basic characteristics of Python, such as:
\begin{itemize}
	\item Why Python?
	\item How does Python relate to software you might be familiar with, such as SPSS, STATA, or R?
	\item An introduction to data types -- or why a ``variable'' is not what you might think
\end{itemize}
Finally, we'll play around a bit with different Python interpreters.


\subsection*{Afternoon (13.30--16.30)}
\textsc{\ding{52} Chapter 4: The very, very basics of programming in Python}\\
~\\
In the afternoon, you will learn about the basics of programming. We will talk about functions, methods, loops, and much more.



\section*{Tuesday, 12-7-2017}


\subsection*{Morning (9.30--12.30)}
\textsc{\ding{52} \cite{Boumans2016}}\\
\textsc{\ding{52} \cite{Gonzalez-Bailon2015}}\\
\textsc{\ding{52} Chapter 6: Sentiment analysis}\\
~\\
We will start with an overview about different approaches to automated content analysis (ACA), as outlined by \cite{Boumans2016}. 

We will discuss sentiment analysis, as it is one of the frequently applied out-of-the-box techniques for analyzing text. We will discuss the simple word count methods, off-the-shelf modules such as Vader \citep{Hutto2014} and Pattern \citep{DeSmedt2012}, and briefly talk about more sophisticated alternatives.




\subsection*{Afternoon (13.30--16.30)}
\textsc{\ding{52} Chapter 7: Automated content analysis}\\
~\\
After that, we will move on to techniques that can be better tailored to your own research questions. Text as written by humans usually is pretty messy. You will therefore learn how to process text to make it suitable for further analysis by using techniques of Natural Language Processing (NLP), and how to extract meaningful information (discarding the rest) using regular expressions. Also, I will introduce you to techniques and concepts like stemming, stopword removal, n-grams, word counts and word co-occurrances.






\section*{Monday, 25-5-2017}
\subsection*{Morning (9.30--12.30)}
\textsc{\ding{52} Chapter 10: Supervised machine learning}\\
~\\
In the first meeting, we considered techniques that are rule-based and, in general, deterministic: ``if you encounter X, do Y'', ``if you find string X, code as mention of actor A''. While such techniques can be a informative and are necessary to automatically and efficiently perform routine tasks, they are less suitable for offering really deep insights into what texts are about.

In this session, I will therefore introduce you to one of the most fascinating topics in automated content analysis: Machine learning. 

I will walk you trough the ideas behind supervised machine learning. The nice thing is that you actually have already done it before: regression analysis a form of supervised ML -- you just never called it like this. And you probably never thought about using such a technique to analyze texts (or images). And that's what we are going to do, using one of the most commonly used frameworks for this, the Python package \emph{scikit-learn} \citep{scikit-learn}.

\subsection*{Afternoon (13.30--16.30)}
\textsc{\ding{52} Chapter 11: Unsupervised machine learning}\\
~\\
In the morning session, we learned a technique for analyzing data assuming that we have a labeled training dataset. But what if we don't? That's where unsupervised machine learning comes into play. Also this is a principle you know already: Principal component analysis is a form of unsupervised ML. We will use a more sophisticated model, though, Latent Dirichlet Allocation (LDA) -- which is a form of so-called topic modelling. We will use the Python package \emph{gensim} \citep{Rehurek2010} as well as the visualization module \emph{pyldavis} \citep{Sievert2014}.





\section*{Tuesday, 26-5-2017}
\subsection*{Morning (9.30--12.30)}
\subsection*{Afternoon (13.30--16.30)}
\textsc{\ding{52} Chapter 8: Web scraping}\\
~\\
We will conclude the workshop with a whole day of what might be the most challenging part: getting actual data! We will look into how to scrape, for instance, review sites and similar online services.




\section*{Further readings}
There are a lot of online ressources available for using Python in the social sciences, just google it. 

Some jupyter notebooks that might be interestig for you are available at \url{http://damiantrilling.net/tools}.
%
%Suggested additional literature on sentiment analysis: 
%\begin{itemize}
%	\item Some  examples of sentiment analysis: \cite{Huang2007,Pestian2012,Mostafa2013}. 
%	\item If you want to have a look under the hood of popular sentiment analysis algorithms, you can read \cite{Thelwall2012} and \cite{Hutto2014}.
%\end{itemize}
%


Next to this, the following books provide the interested participants with more and deeper information. They are intended for the advanced reader and might be very useful for those who want to go deeper into the topic. Keep in mind that, due to the rapidly changing nature of the subject, parts of them are outdated already.

\begin{itemize}
	\item \citealp{Russel2013}. Gives a lot of examples about how to analyze a variety of online data, including Facebook and Twitter, but going much beyond that.
	\item \citealp{Bird2009}. This is the official documentation of the NLTK package that we are using. A newer version of the book can be read for free at \url{http://nltk.org}
	\item \citealp{McKinney2012}: Another book with a lot of examples. A PDF of the book can be downloaded for free on \url{http://it-ebooks.info/book/1041/}.
\end{itemize}





\bibliographystyle{apacite}
\bibliography{../bigdata}

\end{document}